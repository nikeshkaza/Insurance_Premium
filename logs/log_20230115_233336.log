[2023-01-15 23:33:39,750] 	INFO 	29 	training.py 	get_pipeline_config() 	Pipeline configuration: PipelineConfig(pipeline_name='insurance-premium', artifact_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact')
[2023-01-15 23:33:39,750] 	INFO 	66 	training.py 	get_data_ingestion_config() 	Data ingestion config: DataIngestionConfig(data_ingestion_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336', download_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/downloaded_files', file_name='insurance_premium', feature_store_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/feature_store', failed_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/failed_downloaded_files', datasource_url='mongodb+srv://nikeshkaza:OWUCXgQaZkCnE4ht@cluster0.fckbdxk.mongodb.net/?retryWrites=true&w=majority')
[2023-01-15 23:33:39,750] 	INFO 	39 	data_ingestion.py 	__init__() 	>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Starting data ingestion.<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[2023-01-15 23:33:39,751] 	INFO 	110 	data_ingestion.py 	initiate_data_ingestion() 	Started downloading json file
[2023-01-15 23:33:39,751] 	INFO 	53 	data_ingestion.py 	export_data_into_feature_store() 	Exporting data from mongodb to feature store
[2023-01-15 23:33:40,397] 	INFO 	116 	data_ingestion.py 	initiate_data_ingestion() 	Converting and combining downloaded json into parquet file
[2023-01-15 23:33:40,398] 	INFO 	89 	data_ingestion.py 	convert_files_to_parquet() 	Parquet file will be created at: /Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/feature_store/insurance_premium
[2023-01-15 23:33:43,473] 	INFO 	128 	data_ingestion.py 	initiate_data_ingestion() 	Data ingestion artifact: DataIngestionArtifact(feature_store_file_path='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/feature_store/insurance_premium', download_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/downloaded_files')
[2023-01-15 23:33:43,474] 	INFO 	86 	training.py 	get_data_validation_config() 	Data preprocessing config: DataValidationConfig(accepted_data_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_validation/20230115_233336/accepted_data', rejected_data_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_validation/20230115_233336/rejected_data', file_name='insurance_premium')
[2023-01-15 23:33:43,474] 	INFO 	151 	data_validation.py 	initiate_data_validation() 	Initiating data validation
[2023-01-15 23:33:43,575] 	INFO 	51 	data_validation.py 	read_data() 	Data frame is created using file: /Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_ingestion/20230115_233336/feature_store/insurance_premium
[2023-01-15 23:33:43,703] 	INFO 	52 	data_validation.py 	read_data() 	Number of row: 1338 and column: 7
[2023-01-15 23:33:43,703] 	INFO 	62 	data_validation.py 	get_missing_report() 	Preparing missing reports for each column
[2023-01-15 23:33:44,309] 	INFO 	72 	data_validation.py 	get_missing_report() 	Missing report prepared: {'age': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'sex': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'bmi': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'children': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'smoker': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'region': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0), 'expenses': MissingReport(total_row=1338, missing_row=0, missing_percentage=0.0)}
[2023-01-15 23:33:44,309] 	INFO 	161 	data_validation.py 	initiate_data_validation() 	Saving preprocessed data.
[2023-01-15 23:33:44,525] 	INFO 	174 	data_validation.py 	initiate_data_validation() 	Data validation artifact: [DataValidationArtifact(accepted_file_path='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_validation/20230115_233336/accepted_data/insurance_premium', rejected_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_validation/20230115_233336/rejected_data')]
[2023-01-15 23:33:44,526] 	INFO 	115 	training.py 	get_data_transformation_config() 	Data transformation config: DataTransformationConfig(file_name='insurance_premium', export_pipeline_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_transformation/20230115_233336/transformed_pipeline', transformed_train_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_transformation/20230115_233336/train', transformed_test_dir='/Users/nikeshkaza/data_science/POC/insurance_premium/insurance_artifact/data_transformation/20230115_233336/test', test_size=0.3)
[2023-01-15 23:33:44,526] 	INFO 	71 	data_transformation.py 	initiate_data_transformation() 	>>>>>>>>>>>Started data transformation <<<<<<<<<<<<<<<
[2023-01-15 23:33:44,621] 	INFO 	74 	data_transformation.py 	initiate_data_transformation() 	Number of row: [1338] and column: [7]
[2023-01-15 23:33:44,621] 	INFO 	77 	data_transformation.py 	initiate_data_transformation() 	Splitting dataset into train and test set using ration: 0.7:0.3
[2023-01-15 23:33:44,756] 	INFO 	79 	data_transformation.py 	initiate_data_transformation() 	Train dataset has number of row: [945] and column: [7]
[2023-01-15 23:33:44,839] 	INFO 	82 	data_transformation.py 	initiate_data_transformation() 	Train dataset has number of row: [945] and column: [7]
